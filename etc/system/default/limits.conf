# Copyright (C) 2005-2011 Splunk Inc. All Rights Reserved.  Version 4.3.1
# DO NOT EDIT THIS FILE!
# Please make all changes to files in $SPLUNK_HOME/etc/system/local.
# To make changes, copy the section/stanza you want to change from $SPLUNK_HOME/etc/system/default
# into ../local and edit there.
#
# This file configures various limits to the Splunk's search commands.
# CAUTION: Do not alter the settings in limits.conf unless you know what you are doing. 
# Improperly configured limits may result in splunkd crashes and/or memory overuse.


[searchresults]
maxresultrows = 50000
# maximum number of times to try in the atomic write operation (1 = no retries)
tocsv_maxretry = 5
# retry period is 1/2 second (500 milliseconds)
tocsv_retryperiod_ms = 500

[subsearch]
# maximum number of results to return from a subsearch
maxout = 10000
# maximum number of seconds to run a subsearch before finalizing
maxtime = 60
# time to cache a given subsearch's results
ttl = 300

[lookup]
# maximum size of static lookup file to use a in-memory index for
max_memtable_bytes = 10000000
# maximum matches for a lookup
max_matches = 1000
# maximum reverse lookup matches (for search expansion)
max_reverse_matches = 50
# default setting for if non-memory file lookups (for large files) should batch queries
# can be override via a lookup table's stanza in transforms.conf
batch_index_query = true
# when doing batch request, what's the most matches to retrieve
# if more than this limit of matches would otherwise be retrieve, we will fall back to non-batch mode matching
batch_response_limit = 5000000

[anomalousvalue]
maxresultrows = 50000
# maximum number of distinct values for a field
maxvalues = 0
# maximum size in bytes of any single value (truncated to this size if larger)
maxvaluesize = 0

[associate]
maxfields = 10000
maxvalues = 0
maxvaluesize = 0

[autoregress]
maxp = 10000
maxrange = 1000

[concurrency]
# maximum concurrency level to keep record of
max_count = 10000000

# for the contingency, ctable, and counttable commands
[ctable]
maxvalues = 1000

[correlate]
maxfields = 1000

# for bin/bucket/discretize
[discretize]
maxbins = 50000 
# if maxbins not specified or = 0, defaults to searchresults::maxresultrows

[inputcsv]
# maximum number of retries for creating a tmp directory (with random name in SPLUNK_HOME/var/run/splunk)
mkdir_max_retries = 100

[indexpreview]
# maximum number of bytes to read from each file during preview
max_preview_bytes = 2000000
# maximum number of results to emit per call to preview data generator
max_results_perchunk = 2500
# loosely-applied maximum on number of preview data objects held in memory
soft_preview_queue_size = 100

[join]
subsearch_maxout = 50000
subsearch_maxtime = 60
subsearch_timeout = 120

[kmeans]
maxdatapoints = 100000000
maxkvalue = 1000
maxkrange = 100

[typeahead]
maxcount          = 1000
fetch_multiplier  = 50
use_cache         = true
cache_ttl_sec     = 300
min_prefix_length = 1
max_concurrent_per_user = 3

[kv]
# when non-zero, the point at which kv should stop creating new columns
maxcols  = 512
# maximum number of keys auto kv can generate
limit    = 50
# truncate _raw to to this size and then do auto KV
maxchars = 10240

max_extractor_time = 1000
avg_extractor_time = 500

[metadata]
maxresultrows = 10000
# the most metadata results to fetch from each indexer.
maxcount = 100000

[metrics]
# the number of series to include in the per_x_thruput reports in metrics.log
maxseries = 10

[rare]
maxresultrows = 50000
# maximum distinct value vectors to keep track of
maxvalues = 0
maxvaluesize = 0

[restapi]
# maximum result rows to be return by /events or /results getters from REST API  
maxresultrows = 50000

# regex constraint on time_format and output_time_format for search endpoints
time_format_reject = [<>!]

[search]
# how long searches should be stored on disk once completed
ttl = 600

# how long should searches run for a search head live on the indexers
remote_ttl = 600

# by default, no timeline information is retained.  UI will supply the status_buckets as needed
status_buckets = 0

# the maximum number of end results to store (per bucket and globally)
max_count = 500000

# truncate report output to max_count?
truncate_report = false

# the minimum length of a prefix before a * to ask the index about
min_prefix_len = 1

# the largest "_raw" volume that should be read in memory
max_results_raw_size = 100000000

# the length of time to persist search cache entries (in seconds)
cache_ttl = 300

# maximum results per call to search (in dispatch), must be <= maxresultrows
max_results_perchunk = 2500

# minimum reuslts per call to search (in dispatch), must be <= max_results_perchunk
min_results_perchunk = 100

# maximum raw size of results per call to search (in dispatch), 0 = no limit
max_rawsize_perchunk = 5000000

# target duration of a particular call to fetch search results in ms
target_time_perchunk = 2000

# time in seconds until a search is considered "long running"
long_search_threshold = 2

# max_results_perchunk, min_results_perchunk, and target_time_perchunk 
# are multiplied by this for a long running search
chunk_multiplier = 5

# the minimum frequency of a field displayed in the /summary endpoint
min_freq = 0.01

# the frequency with which try to reduce intermediate data when there is an non-streaming and non-stateful streaming command. (0 = never)
reduce_freq = 10

# the maximum time to spend doing reduce, as a fraction of total search time
reduce_duty_cycle = 0.25

# the maximum time to spend generating previews, as a fraction of total search time
preview_duty_cycle = 0.25

# the maximum number of times to retry to dispatch a search when the quota has been reached
dispatch_quota_retry = 4

# milliseconds between retrying to dispatch a search if a quota has been reached
# we retry the given number of times, with each successive wait 2x longer than the previous
dispatch_quota_sleep_ms = 100

# the maximum number of concurrent searches per CPU 
max_searches_per_cpu = 4

# the base number of concurrent searches
base_max_searches = 4

# max real-time searches = max_rt_search_multiplier x max historical searches
max_rt_search_multiplier = 3

# the total number of concurrent searches is base_max_searches + #cpus*max_searches_per_cpu

# max recursion depth for macros 
# considered a search exception if macro expansion doesn't stop after this many levels
max_macro_depth = 100

# for real-time searches in the UI, maximum number of events stored (as a FIFO buffer)
realtime_buffer = 10000

# stack size of the search executing thread
stack_size = 4194304

# the number of search job metadata to cache in RAM
status_cache_size = 10000

# search results combiner maximum in-memory buffer size (in events)
max_combiner_memevents = 50000

# the minimum bundle replication period
replication_period_sec  = 60

# whether bundle replication is synchronous (and thus blocking searches)
sync_bundle_replication = auto

# whether to use multiple threads when setting up distributed search to multiple peers
multi_threaded_setup = false

# when doing round-robin fetching, what are the min,max,and backoff factors for the amount
# of time (in ms) to sleep when no data is available
rr_min_sleep_ms = 10
rr_max_sleep_ms = 1000
rr_sleep_factor = 2

# how often to update the field summary statistics, as a ratio to the elapsed run time so far
fieldstats_update_freq = 0
# maximum period for updating field summary statistics in seconds
fieldstats_update_maxperiod = 60

# allow timeline to be map/reduced?
remote_timeline = true
# minimum peers required to utlize remote timelining
remote_timeline_min_peers = 1
# fetch all timeline accessible events from the remote peers?
remote_timeline_fetchall = false
# use asynchronous thread to fetch remote events?
remote_timeline_thread = true
# how often to touch remote artifacts to keep them from being reaped when search has not finished? (in seconds)
remote_timeline_touchperiod = 300

# max number of events per bucket to be stored on each peer for remote timeline
remote_timeline_max_count = 10000
# max space that remote timeline can use on each peer, in MB
remote_timeline_max_size_mb = 1000

# timeouts for fetching remote timeline events
remote_timeline_connection_timeout = 5
remote_timeline_send_timeout = 10
remote_timeline_receive_timeout = 10

# default setting for allowing async jobs to be queued if quota violation
default_allow_queue = false

# how often to retry queued jobs (in seconds)
queued_job_check_freq = 10

# enable search history?
enable_history = true

# max number of searches to store in history (per user/app)
max_history_length = 1000

# allow inexact metasearch?
allow_inexact_metasearch = false

# number of jobs in dispatch dir before a warning is issued
dispatch_dir_warning_size = 2000

[realtime] 
# default options for indexer support of real-time searches
# these can all be overriden for a single search via REST API arguments

# size of queue for each real-time search
queue_size = 10000

# should indexer block if a queue is full?
blocking = false

# maximum time to block if the queue is full (meaningless if blocking = false)
max_blocking_secs = 60

# should the indexer prefilter events for efficiency?
indexfilter = true

# should real-time windowed searches backfill with historical data by default?
default_backfill = true

# should real-time windowed searches sort events to be in descending time order?
enforce_time_order = true

[set]
maxresultrows = 50000


[slc]
# maximum number of clusters to create
maxclusters = 10000

[sort]
# maximum number of concurrent files to open
maxfiles = 64

[spath]
# number of characters to read from an XML or JSON event when auto extracting
extraction_cutoff = 5000

[stats]
maxresultrows = 50000
maxvalues = 0
maxvaluesize = 0
# for streamstats's maximum window size
max_stream_window = 10000
# for rdigest, used to approximate order statistics (median, percentiles)
rdigest_k = 100
rdigest_maxnodes = 1

[sistats]
maxvalues = 0
maxvaluesize = 0
rdigest_k = 100
rdigest_maxnodes = 1
max_valuemap_bytes = 100000

[top]
maxresultrows = 50000
# maximum distinct value vectors to keep track of
maxvalues = 0
maxvaluesize = 0


[transactions]
# maximum number of open transaction or events in open
# transaction before transaction eviction happens
maxopentxn    = 5000
maxopenevents = 100000


[typer]
# in eventtyping, pay attention to first N characters of any
# attribute (e.g., _raw), including individual tokens. Can be
# overridden by supplying the typer operator with the argument 
# maxlen (e.g. "|typer maxlen=300").
maxlen = 10000


[authtokens]
# Time before an auth token will be expire if not used, in seconds.
# 0 will indicate no timeout.
expiration_time = 3600

[sample]
maxsamples = 10000
maxtotalsamples = 100000


[scheduler]
# the maximum number of searches the scheduler can run, as a percentage
# of the maximum number of concurrent searches 
max_searches_perc  = 25

# maximum number of results to load when triggering an action
max_action_results = 50000

action_execution_threads  = 2

actions_queue_size        = 100

alerts_max_count          = 50000 

alerts_expire_period      = 120

peristance_period         = 30

# maximum number of lock files to keep around for each scheduled search
# effective only if search head pooling is enabled, the most recent files are kept
max_lock_files = 5

# the lock file reaper should clean lock files that are this old (in seconds)
max_lock_file_ttl = 86400

max_per_result_alerts = 500

[thruput]
# throughput limiting at index time
maxKBps = 0

[show_source]
# maximum events retriveable by show source
max_count = 10000
max_timebefore = 1day
max_timeafter = 1day
distributed = true
# maximum events we will request in the distributed show source. Likely all of these will not be used
distributed_search_limit = 30000

[ldap]
# maximum number of users we will attempt to precache from LDAP after reloading auth
max_users_to_precache = 1000
# controls whether we allow login when we find multiple entries with the same value for the username attribute
allow_multiple_matching_users = true

[reversedns]
# max percent of time allowed for reverse dns lookups for incoming 
# forwarder connections before WARN is logged in splunkd.log
# sanity check diagnostic for slow lookups
rdnsMaxDutyCycle = 10

[viewstates]
# is the viewstate reaper enabled?
enable_reaper = true
# how often does the reaper run?
reaper_freq = 86400
# how many viewstates does the reaper consider "acceptable"?
reaper_soft_warn_level = 1000
# reaper eligibility age
ttl = 86400
