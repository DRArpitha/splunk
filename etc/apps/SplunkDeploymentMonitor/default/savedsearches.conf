#########################
# BEGIN HOMEPAGE SEARCHES
#########################


## FlashChart stuff

[DM indexthru today vs last week]
enableSched = 1
cron_schedule = 0 * * * * 
description = Index Throughput Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +1d@d-30m
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

#TODO - timechart before append + preview = EVIL
search = index="summary_indexers" | timechart partial=f span=30m per_second(kb) as KBps | eval marker = "today" | eval _time = _time+1800 | append [search index="summary_indexers" earliest=-7d@d-30m latest=-6d@d-30m | timechart span=30m per_second(kb) as KBps | eval marker = "this day last week" | eval _time = _time+86400*7+1800] | timechart median(KBps) by marker 


[DM indexthru week over week]
enableSched = 1
cron_schedule = 0 * * * *
description = Indexing Throughput Week Over Week
dispatch.earliest_time = -13d@d-30m 
dispatch.latest_time = +1d@d-30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

#instead of addinfo you could also use the relative functions in eval.  but this seems fine too. 
search = index="summary_indexers" | timechart partial=f span=30m per_second(kb) as KBps | addinfo | eval marker = if(_time < info_min_time + 7*86400, "last week", "this week") | eval _time = if(_time < info_min_time + 7*86400, _time + 7*86400+1800, _time+1800) | chart median(KBps) by _time marker 


[DM tcpin today vs last week]
enableSched = 1
cron_schedule = */30 * * * *
description = Distinct Connections Today vs Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +1d@d-30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

#TODO - timechart before append + preview = EVIL
#NOTE: maybe because this is a dc(sourceIp) rather than a "foo by sourceIp", we avoid the crushing density problem at high volumes, 
#      and we can maybe get away with this search...
search = index="summary_forwarders" | timechart partial=f span=30m dc(guid) as distcount | eval marker = "today" | eval _time = _time+1800 | append [search earliest=-7d@d-30m latest=-6d@d-30m index="summary_forwarders" | timechart span=30m dc(guid) as distcount| eval marker = "this day last week" | eval _time = _time+86400*7+1800] | timechart median(distcount) by marker |  eval today=if(_time>now()-60,null(), today)




## SingleValue stuff

[DM idle indexers]
enableSched = 1
cron_schedule = */30 * * * *
description = Idle Indexers
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
displayview = flashtimeline
request.ui_dispatch_view = indexer_list
#TODO - if kb=0, the indexer just doesnt write anything. 
# switch to inputlookup.  keep lookup of (splunk_server, connectionType)  
# Anything in there and not in last15mins search is ?assumed? to be 'idle'
#search = `per_index_metrics` |  join type="outer" splunk_server [search `indexer_active_searches`] | stats sum(kb) as kb by splunk_server | search kb=0 | eval kb=round(kb, 4) | fields splunk_server kb activesearches | fillnull activesearches | rename splunk_server as "Splunk Server" kb as "Total KB" activesearches as "Active Ongoing Searches"
search = `all_indexers` | search status = "idle" | fields splunk_server KB status | rename splunk_server as "Splunk Server" KB as "Total KB" status as "Current Status"


[DM loaded indexers]
enableSched = 1
cron_schedule = */30 * * * *
description = Number of Indexers Whose Queue/s Seem Loaded
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
displayview = indexer_list
request.ui_dispatch_view = indexer_list
#search = `indexer_parsing_queue` | join type="outer" splunk_server [search `indexer_active_searches`] | search percentage > 50 | fillnull activesearches | fields splunk_server p95sz percentage activesearches | rename splunk_server as "Splunk Server"  p95sz as "95th Percentile Queue Size" percentage as "95th Percentile As Fraction of Max Queue Size" activesearches as "Active Ongoing Searches"
search = `all_indexers` | search status="overloaded" | fields splunk_server avg_age parseQ_percentage indexQ_percentage status | rename splunk_server as "Splunk Server"  avg_age as "Average Latency (in Seconds)" parseQ_percentage as "Parsing Queue 95th Percentile As Fraction of Max Queue Size" indexQ_percentage as "Index Queue 95th Percentile As Fraction of Max Queue Size" status as "Current Status"


[DM missing forwarders]
enableSched = 1
cron_schedule = */15 * * * *
description = Forwarders that we've seen at some point in history, but not recently.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
displayview = forwarder_list
request.ui_dispatch_view = forwarder_list
search = `all_forwarders` | search status="missing" | sort - lastConnected | fields sourceHost sourceIp connectionType lastConnected status | rename sourceHost as "Forwarder" sourceIp as "Source IP" connectionType as "Forwarder Type" lastConnected as "Last Connected" status as "Current Status"  | fieldformat "Last Connected"=strftime('Last Connected', "%D %H:%M:%S %p")


[DM quiet forwarders]
enableSched = 1
cron_schedule = */15 * * * *
description = Forwarders who have connected recently but havent sent any data recently
dispatch.earliest_time = -1d@d
dispatch.latest_time = now
displayview = forwarder_list
request.ui_dispatch_view = forwarder_list
search =  `all_forwarders` | search status="quiet" | sort - lastReceived | eval eps = round(eps,4) | fields sourceHost sourceIp connectionType lastConnected lastReceived KB eps status | rename sourceHost as "Forwarder" sourceIp as "Source IP" connectionType as "Forwarder Type" lastConnected as "Last Connected" lastReceived as "Last Data Received" KB as "Total KB" eps as "Average Events Per Second" status as "Current Status"  | fieldformat "Last Connected"=strftime('Last Connected', "%D %H:%M:%S %p")  | fieldformat "Last Data Received"=strftime('Last Data Received', "%D %H:%M:%S %p")



[DM forwarders too little data]
enableSched = 1
cron_schedule = */30 * * * *
description = Forwarders Sending a Lot Less Data than Expected
dispatch.earliest_time = -1h@h
dispatch.latest_time = now 
displayview = forwarder_list
request.ui_dispatch_view = forwarder_list
#1) obtain a scaling factor that's "all indexer activity from past hour a week ago" / all indexer activity past hour",
#2) compare "this forwarder metrics from past hour exactly a week ago" / "this forwarder metrics from past hour"
#3) apply scaling factor.    
search = `forwarder_metrics` | `forwarder_metrics_stats` avg(kb) as avg_kb_today by guid  | join guid type=outer [search earliest=-169h@h latest=-168h@h `forwarder_metrics`| stats avg(kb) as avg_kb_last_week by guid] | fillnull avg_kb_today avg_kb_last_week | appendcols [search `indexer_ratio` | fields indexer_ratio] | streamstats first(indexer_ratio) as indexer_ratio |  where indexer_ratio*.5*avg_kb_last_week > avg_kb_today | eval kb_diff = abs(round(avg_kb_last_week - avg_kb_today, 4)) | eval kb_diff_perc = round(100*kb_diff/avg_kb_last_week, 4) | eval avg_kb_last_week = round(avg_kb_last_week, 4) | eval avg_kb_today = round(avg_kb_today, 4) |  fields sourceHost connectionType avg_kb_last_week avg_kb_today kb_diff kb_diff_perc | rename sourceHost as "Forwarder" avg_kb_last_week as "Average KBps Last Week" avg_kb_today as "Average KBps Today" connectionType as "Forwarder Type"  kb_diff as "KBps Difference from Last Week" kb_diff_perc as "Percentage Difference" 

[DM forwarders too much data]
enableSched = 1
cron_schedule = */30 * * * *
description = Forwarders That Are Sending More Data Than Expected
dispatch.earliest_time =-1h@h 
dispatch.latest_time = now 
displayview = forwarder_list
request.ui_dispatch_view = forwarder_list
# same scaling factor strategy
search = `forwarder_metrics` | `forwarder_metrics_stats` avg(kb) as avg_kb_today by guid  | join guid type=outer [search earliest=-169h@h latest=-168h@h `forwarder_metrics`| stats avg(kb) as avg_kb_last_week by guid] | fillnull avg_kb_today avg_kb_last_week | appendcols [search `indexer_ratio` | fields indexer_ratio] | streamstats first(indexer_ratio) as indexer_ratio |  where indexer_ratio*avg_kb_last_week < .5*avg_kb_today | eval kb_diff = abs(round(avg_kb_last_week - avg_kb_today, 4)) | eval kb_diff_perc = round(100*kb_diff/avg_kb_last_week, 4) | eval avg_kb_last_week = round(avg_kb_last_week, 4) | eval avg_kb_today = round(avg_kb_today, 4) |  fields sourceHost connectionType avg_kb_last_week avg_kb_today kb_diff kb_diff_perc | rename sourceHost as "Forwarder" avg_kb_last_week as "Average KBps Last Week" avg_kb_today as "Average KBps Today" connectionType as "Forwarder Type"  kb_diff as "KBps Difference from Last Week" kb_diff_perc as "Percentage Difference" 


####################
# ALL "FOO" SEARCHES
####################

[All forwarders]
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
displayview = forwarder_list
request.ui_dispatch_view = forwarder_list
search = `all_forwarders`


[All forwarders - regenerator summary index]
is_visible = false
enableSched = 1
cron_schedule = */30 * * * *
description = This checks every 30 mins and updates/adds forwarders to summary index 
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
search = `forwarder_metrics` |  eval lastReceived = if(kb>0, _time, null) | `forwarder_lookup_stats("max(_time) as lastConnected max(lastReceived) as lastReceived sum(kb) as kb avg(tcp_eps) as avg_eps")`
action.summary_index = 1
action.summary_index._name = summary_forwarders   
action.summary_index.report = "DM forwarder summary index"


[All indexers]
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
displayview = indexer_list
request.ui_dispatch_view = indexer_list
search = `all_indexers`


[All indexers - regenerator]
is_visible = false
enableSched = 1
cron_schedule = */30 * * * *
description = This checks every 30 mins and updates/adds indexers to the summary index 
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
search = `per_index_metrics` | stats sum(kb) as kb by splunk_server | join type="outer" splunk_server [ search `indexer_queue_stats`] | rename splunk_server as my_splunk_server
#search = `per_index_metrics` | stats sum(kb) as kb by splunk_server | join type="outer" splunk_server [ search `indexer_parsing_queue`] | rename splunk_server as my_splunk_server
action.summary_index = 1
action.summary_index._name = summary_indexers   
action.summary_index.report = "DM indexer summary index"


[All indexers timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Index Throughput Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = now 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_indexers" | eval mb=kb/1024 | eval _time = _time+1800 | rename my_splunk_server as splunk_server | timechart partial=f sum(mb) as MB by splunk_server 


####################
# MISC
####################

[DM indexer queue perc95 timechart]
description = Timechart of 95th Percentile Current Size over Max Size of indexer Queue/s
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display
search = index="_internal" source="*metrics.log" group=queue name=parsingqueue | timechart partial=f eval(perc95(current_size)*100/max(max_size)) by splunk_server

[DM kb indexed last N]
description = Sum KB over all indexes in past 15 minutes
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
displayview = flashtimeline
request.ui_dispatch_view = flashtimeline
search = index="_internal" source="*metrics.log" group="per_index_thruput" | stats sum(kb) as kb


####################
# New searches from licenser logs
####################


[DM missing sourcetypes]
enableSched = 1
cron_schedule = */15 * * * *
description = Missing Sourcetypes
dispatch.earliest_time = -1h@h
dispatch.latest_time = now 
displayview = sourcetype_list
request.ui_dispatch_view = sourcetype_list 
search = `all_sourcetypes`  | search status="missing" | sort -lastReceived | fields lastReceived mysourcetype bytes status | rename lastReceived as "Last Connected" mysourcetype as "Sourcetype" bytes as "Bytes" status as "Status" | fieldformat "Last Connected"=strftime('Last Connected', "%D %H:%M:%S %p") 

[All sourcetypes]
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
displayview = sourcetype_list
request.ui_dispatch_view = sourcetype_list
search = `all_sourcetypes`

[All sourcetypes - regenerator]
is_visible = false
enableSched = 1
cron_schedule = */30 * * * *
description = This checks every 30 mins and updates/adds sourcetypes to the summary index 
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
search = `sourcetype_metrics` | fillnull bytes | stats sum(bytes) as bytes max(lastReceived) as lastReceived by mysourcetype |  appendcols [search earliest=@d `all_sourcetypes` | stats sum(bytes) as bytes_today by mysourcetype | fields bytes_today] | where isnotnull(mysourcetype) |  rename mysourcetype as my_sourcetype

action.summary_index = 1
action.summary_index._name = summary_sourcetypes   
action.summary_index.report = "DM sourcetypes summary index"


[DM sourcetypes too little data]
enableSched = 1
cron_schedule = */30 * * * *
description = Sourcetypes Sending a Lot Less Data than Expected
dispatch.earliest_time = -1h@h
dispatch.latest_time = now 
displayview = sourcetype_list
request.ui_dispatch_view = sourcetype_list 
#1) obtain a scaling factor that's "all indexer activity from past hour a week ago" / all indexer activity past hour",
#2) compare "this sourcetype metrics from past hour exactly a week ago" / "this sourcetype metrics from past hour"
#3) apply scaling factor.    
search = `sourcetype_metrics`  | stats avg(bytes) as avg_bytes_today by mysourcetype  | join mysourcetype type=outer [search earliest=-169h@h latest=-168h@h `sourcetype_metrics`| stats avg(bytes) as avg_bytes_last_week by mysourcetype] | fillnull avg_bytes_today avg_bytes_last_week | eval avg_bytes_today=round(avg_bytes_today) | eval avg_bytes_last_week=round(avg_bytes_last_week) | appendcols [search `indexer_ratio` | fields indexer_ratio] | streamstats first(indexer_ratio) as indexer_ratio |  where indexer_ratio*.5*avg_bytes_last_week > avg_bytes_today | eval bytes_diff = abs(avg_bytes_last_week - avg_bytes_today) | eval bytes_diff_perc = round(100*bytes_diff/avg_bytes_last_week, 4) | fields mysourcetype avg_bytes_last_week avg_bytes_today bytes_diff bytes_diff_perc | rename mysourcetype as "Sourcetype" avg_bytes_last_week as "Average Bytes Last Week" avg_bytes_today as "Average Bytes Today" bytes_diff as "Bytes Difference from Last Week" bytes_diff_perc as "Percentage Difference" 


[DM sourcetypes too much data]
enableSched = 1
cron_schedule = */30 * * * *
description = Sourcetypes Sending a Lot More Data than Expected
dispatch.earliest_time = -1h@h
dispatch.latest_time = now 
displayview = sourcetype_list
request.ui_dispatch_view = sourcetype_list 
#1) obtain a scaling factor that's "all indexer activity from past hour a week ago" / all indexer activity past hour",
#2) compare "this sourcetype metrics from past hour exactly a week ago" / "this sourcetype metrics from past hour"
#3) apply scaling factor.    
search = `sourcetype_metrics`  | stats avg(bytes) as avg_bytes_today by mysourcetype  | join mysourcetype type=outer [search earliest=-169h@h latest=-168h@h `sourcetype_metrics`| stats avg(bytes) as avg_bytes_last_week by mysourcetype] | fillnull avg_bytes_today avg_bytes_last_week | eval avg_bytes_today=round(avg_bytes_today) | eval avg_bytes_last_week=round(avg_bytes_last_week) | appendcols [search `indexer_ratio` | fields indexer_ratio] | streamstats first(indexer_ratio) as indexer_ratio |  where indexer_ratio*avg_bytes_last_week < 0.5*avg_bytes_today | eval bytes_diff = abs(avg_bytes_last_week - avg_bytes_today) | eval bytes_diff_perc = round(100*bytes_diff/avg_bytes_last_week, 4) | fields mysourcetype avg_bytes_last_week avg_bytes_today bytes_diff bytes_diff_perc | rename mysourcetype as "Sourcetype" avg_bytes_last_week as "Average Bytes Last Week" avg_bytes_today as "Average Bytes Today" bytes_diff as "Bytes Difference from Last Week" bytes_diff_perc as "Percentage Difference" 


[All sourcetypes timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Sourcetypes Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +30m
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_sourcetypes" |  eval Mbytes = bytes/1048576 | eval _time = _time+1800 | rename my_sourcetype as mysourcetype | timechart partial=f sum(Mbytes) as Mbytes by mysourcetype 

[All sourcetypes cumulative timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Sourcetypes Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_sourcetypes" |  eval Mbytes = bytes/1048576 | eval _time = _time+1800 | rename my_sourcetype as mysourcetype | timechart partial=f span=30m avg(Mbytes) as Mbytes by mysourcetype | bin _time as _day span=d | streamstats sum(*) as * by _day 


##### Licenser view searches #####

[All sources]
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
displayview = source_list
request.ui_dispatch_view = source_list
search = `all_sources`

[All sources - regenerator]
is_visible = false
enableSched = 1
cron_schedule = */30 * * * *
description = This checks every 30 mins and updates/adds sources to the summary index 
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
search = `sourcetype_metrics` | fillnull bytes | stats sum(bytes) as bytes max(lastReceived) as lastReceived by source | rename source as my_source 
action.summary_index = 1
action.summary_index._name = summary_sources   
action.summary_index.report = "DM sources summary index"

[All sources timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Sources Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_sources" |  eval Mbytes = bytes/1048576 | eval _time = _time+1800 | rename my_source as source | timechart partial=f sum(Mbytes) as Mbytes by source 

[All hosts]
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
displayview = host_list
request.ui_dispatch_view = host_list
search = `all_hosts`

[All hosts - regenerator]
is_visible = false
enableSched = 1
cron_schedule = */30 * * * *
description = This checks every 30 mins and updates/adds hosts to the summary index 
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
search = `sourcetype_metrics` | fillnull bytes | stats sum(bytes) as bytes max(lastReceived) as lastReceived by host | rename host as my_host 
action.summary_index = 1
action.summary_index._name = summary_hosts   
action.summary_index.report = "DM hosts summary index"


[All hosts timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Hosts Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_hosts" |  eval Mbytes = bytes/1048576 | eval _time = _time+1800 | rename my_host as host | timechart partial=f sum(Mbytes) as Mbytes by host  


[All pools]
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
displayview = pool_list
request.ui_dispatch_view = pool_list
search = `all_pools`

[All pools - regenerator]
is_visible = false
enableSched = 1
cron_schedule = */30 * * * *
description = This checks every 30 mins and updates/adds pools to the summary index 
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
search = `sourcetype_metrics` | fillnull bytes | stats sum(bytes) as bytes max(lastReceived) as lastReceived by pool | rename pool as my_pool 
action.summary_index = 1
action.summary_index._name = summary_pools   
action.summary_index.report = "DM pools summary index"


[All pools timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Hosts Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_pools" |  eval Mbytes = bytes/1048576 | eval _time = _time+1800 | rename my_pool as pool | timechart partial=f sum(Mbytes) as Mbytes by pool  

[All forwarders timechart]
enableSched = 1
cron_schedule = */30 * * * * 
description = Forwarders Today Versus Same Day Last Week
dispatch.earliest_time = @d-30m
dispatch.latest_time = +30m 
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display

search = index="summary_forwarders" | eval mb=kb/1024 | eval _time = _time+1800 | timechart partial=f sum(mb) as MB by sourceHost 

##### Saved Searches to Purge Summary Indexes #####

[All forwarders - purge]
dispatch.earliest_time = -14d@d
dispatch.latest_time = now
displayview = forwarder_list
request.ui_dispatch_view = forwarder_list
search = index="summary_forwarders" | delete

[All indexers - purge]
dispatch.earliest_time = -14d@d
dispatch.latest_time = now
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display
search = index="summary_indexers" | delete


[All sources - purge]
dispatch.earliest_time = -14d@d
dispatch.latest_time = now
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display
search = index="summary_sources" | delete


[All hosts - purge]
dispatch.earliest_time = -14d@d
dispatch.latest_time = now
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display
search = index="summary_hosts" | delete


[All pools - purge]
dispatch.earliest_time = -14d@d
dispatch.latest_time = now
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display
search = index="summary_pools" | delete

[All sourcetypes - purge]
dispatch.earliest_time = -14d@d
dispatch.latest_time = now
displayview = report_builder_display
request.ui_dispatch_view = report_builder_display
search = index="summary_sourcetypes" | delete

######## License Report View ############
[Daily License Usage - last 30 days]
dispatch.earliest_time = -31d@d
dispatch.latest_time = @d
displayview = report_builder_display
cron_schedule = 0 1 * * * 
request.ui_dispatch_view = report_builder_display
search = index="summary_indexers" |  eval _time = _time+1800 |  eval gb=kb/1048576 | bucket _time as day span=d | stats sum(gb) as gb by day | stats max(gb) as peakdailyusage, avg(gb) as avgdailyusage | eval peakdailyusage=round(peakdailyusage,2) | eval avgdailyusage=round(avgdailyusage,2)

[Top Five Daily Usage - last 30 days]
dispatch.earliest_time = -31d@d
dispatch.latest_time = @d
displayview = report_builder_display
cron_schedule = 0 1 * * * 
request.ui_dispatch_view = report_builder_display
search = index="summary_indexers" |  eval _time = _time+1800 |  eval gb=kb/1048576 | bucket _time as day span=d | stats sum(gb) as gb by day | sort limit=5 -gb | stats avg(gb) as mytop5 | eval mytop5=round(mytop5,2)


[Daily License Usage Timechart - last 4 weeks]
dispatch.earliest_time = @w
dispatch.latest_time = +1w@w 
displayview = report_builder_display
cron_schedule = 0 1 * * * 
request.ui_dispatch_view = report_builder_display
search = index="summary_indexers" |  eval _time = _time+1800 | timechart partial=f span=1d sum(kb) as KB | eval marker="this week" | append [search index="summary_indexers" earliest=-1w@w latest=@w | timechart span=1d sum(kb) as KB | eval marker = "prior week" | eval _time = _time+86400*7+1800] | append [search index="summary_indexers" earliest=-2w@w latest=-1w@w | timechart span=1d sum(kb) as KB | eval marker = "2 weeks ago" | eval _time = _time+86400*7*2+1800] | append [search index="summary_indexers" earliest=-3w@w latest=-2w@w | timechart span=1d sum(kb) as KB | eval marker = "3 weeks ago" | eval _time = _time+86400*7*3+1800] | eval gb=KB/1048576 | timechart median(gb) by marker


[Daily License Usage by Indexer - last 60 days]
dispatch.earliest_time = -61d@d
dispatch.latest_time = @d 
displayview = report_builder_display
cron_schedule = 0 1 * * * 
request.ui_dispatch_view = report_builder_display
search = index="summary_indexers" |  eval _time = _time+1800 |  bucket _time as day span=d | eval gb=kb/1048576 | stats sum(gb) as gb by my_splunk_server, day | eval gb=round(gb,2) 

[Daily License Usage by Pool - last 60 days]
dispatch.earliest_time = -61d@d
dispatch.latest_time = @d 
displayview = report_builder_display
cron_schedule = 0 1 * * * 
request.ui_dispatch_view = report_builder_display
search = index="summary_pools" | eval _time = _time+1800 |  bucket _time as day span=d  | eval gb=bytes/1073741824  | stats sum(gb) as gb by my_pool, day | eval gb=round(gb,2) 

